# Importing necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# Load data
def load_data(file_path):
    return pd.read_csv(file_path)

# Preprocess data
def preprocess_data(data):
    # Handling missing values
    data.fillna(method='ffill', inplace=True)

    # Feature Engineering
    # Creating new features based on domain knowledge
    data['Delivery_Speed'] = data['Distance'] / data['Duration']
    data['Fuel_Efficiency'] = data['Distance'] / data['Fuel_Used']

    return data

# Exploratory Data Analysis
def perform_eda(data):
    # Plotting correlations
    plt.figure(figsize=(12, 10))
    sns.heatmap(data.corr(), annot=True, fmt='.2f', cmap='coolwarm')
    plt.title("Feature Correlation Matrix")
    plt.show()

    # Additional EDA plots can be added here

# Data Analysis
def perform_analysis(data):
    # Selecting relevant features for analysis
    features = ['Distance', 'Duration', 'Delivery_Speed', 'Fuel_Efficiency', 'Truck_Type']
    target = 'Cost'

    # Splitting data into features and target
    X = data[features]
    y = data[target]

    # Preprocessing Pipeline
    numerical_features = ['Distance', 'Duration', 'Delivery_Speed', 'Fuel_Efficiency']
    categorical_features = ['Truck_Type']

    # Preprocessing steps for numerical and categorical data
    numerical_transformer = Pipeline(steps=[
        ('scaler', MinMaxScaler())
    ])
    categorical_transformer = Pipeline(steps=[
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_features),
            ('cat', categorical_transformer, categorical_features)
        ])

    # Create the model
    model = Pipeline(steps=[('preprocessor', preprocessor),
                            ('model', RandomForestRegressor(n_estimators=100))])

    # Splitting the dataset for training and testing
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

    # Training the model
    model.fit(X_train, y_train)

    # Predicting and evaluating the model
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    print(f"Mean Squared Error: {mse}")

    return model

# Main function
def main():
    # Load and preprocess data
    data = load_data('path/to/freight_data.csv')
    processed_data = preprocess_data(data)

    # Perform exploratory data analysis
    perform_eda(processed_data)

    # Perform data analysis
    model = perform_analysis(processed_data)

    # Model can be saved or further used here

if __name__ == "__main__":
    main()
